# -*- coding: utf-8 -*-
"""lab11-340.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YLTYcNQ2uV0ek9ejc1OqbJnStztndSff
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

def fine_grained_saliency(image_path):
    image = cv2.imread(image_path)

    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    saliency = cv2.saliency.StaticSaliencyFineGrained_create()
    (success, saliencyMap) = saliency.computeSaliency(image)

    saliencyMap = (saliencyMap * 255).astype("uint8")


    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(image_rgb)
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1, 2, 2)
    plt.imshow(saliencyMap, cmap='gray')
    plt.title("Fine-Grained Saliency")
    plt.axis("off")



    plt.tight_layout()
    plt.show()

fine_grained_saliency('run.jpg')

from google.colab.patches import cv2_imshow
import cv2

def video_saliency_detection(video_path):
    cap = cv2.VideoCapture(video_path)

    saliency = cv2.saliency.MotionSaliencyBinWangApr2014_create()
    saliency.setImagesize(640, 480)
    saliency.init()

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, (640, 480))
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        success, saliencyMap = saliency.computeSaliency(gray)
        saliencyMap = (saliencyMap * 255).astype("uint8")

        print("Frame:")
        print("Saliency Map:")
        cv2_imshow(saliencyMap)

        key = cv2.waitKey(200)
        if key == 27:
            break

    cap.release()
    cv2.destroyAllWindows()

video_saliency_detection('a.mp4')

import numpy as np
import cv2
import matplotlib.pyplot as plt

def kmeans_segmentation(image_path, K=5):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    Z = image.reshape((-1,3))
    Z = np.float32(Z)

    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)
    ret, label, center = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

    center = np.uint8(center)
    res = center[label.flatten()]
    segmented_image = res.reshape((image.shape))

    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plt.imshow(image)
    plt.title('Original Image')
    plt.axis('off')

    plt.subplot(1,2,2)
    plt.imshow(segmented_image)
    plt.title(f'KMeans Segmented (K={K})')
    plt.axis('off')

    plt.show()

kmeans_segmentation('landscape.jpg')

import cv2
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

def graph_cut_segmentation_with_graph(image_path):
    img = cv2.imread(image_path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    h, w = img_gray.shape
    mask = np.zeros((h, w), np.uint8)
    bgdModel = np.zeros((1, 65), np.float64)
    fgdModel = np.zeros((1, 65), np.float64)

    rect = (50, 50, w - 100, h - 100)
    cv2.grabCut(img_rgb, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)
    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype("uint8")
    img_segmented = img_rgb * mask2[:, :, np.newaxis]

    patch_size = 20
    patch = img_gray[0:patch_size, 0:patch_size]

    G = nx.grid_2d_graph(patch_size, patch_size)
    pos = {(i, j): (j, -i) for i in range(patch_size) for j in range(patch_size)}

    for (i, j) in G.nodes():
        if i + 1 < patch_size:
            weight = abs(int(patch[i, j]) - int(patch[i + 1, j]))
            G.add_edge((i, j), (i + 1, j), weight=weight)
        if j + 1 < patch_size:
            weight = abs(int(patch[i, j]) - int(patch[i, j + 1]))
            G.add_edge((i, j), (i, j + 1), weight=weight)

    fig, axs = plt.subplots(1, 3, figsize=(18, 6))

    axs[0].imshow(img_rgb)
    axs[0].set_title("Original Image")
    axs[0].axis("off")

    axs[1].imshow(img_segmented)
    axs[1].set_title("Graph Cut Result")
    axs[1].axis("off")

    edge_weights = nx.get_edge_attributes(G, 'weight')
    nx.draw(G, pos, ax=axs[2], node_size=10, with_labels=False, edge_color=list(edge_weights.values()),
            edge_cmap=plt.cm.plasma)
    axs[2].set_title("Approx. Pixel Graph with Edge Weights")

    plt.tight_layout()
    plt.show()

mask = graph_cut_segmentation_with_graph('cat.jpg')

!pip install imageio tqdm --quiet
!apt-get install unrar -y


import os

dataset_url = "https://www.crcv.ucf.edu/data/UCF101/UCF101.rar"
rar_path = "/content/UCF101.rar"
extract_dir = "/content/UCF-101"
os.makedirs(extract_dir, exist_ok=True)

!curl --insecure -o {rar_path} {dataset_url}

!unrar x {rar_path} {extract_dir}

import cv2
import numpy as np
from glob import glob
from tqdm import tqdm
import imageio

def extract_frames_from_video(video_path, target_frames=16, size=(64, 64)):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret: break
        frame = cv2.resize(frame, size)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame)
    cap.release()

    if len(frames) >= target_frames:
        frames = frames[:target_frames]
    else:
        while len(frames) < target_frames:
            frames.append(frames[-1])
    return np.array(frames) / 255.0

jumpingjack_videos = sorted(glob("/content/UCF-101/UCF-101/JumpingJack/*.avi"))[:5]
pushups_videos = sorted(glob("/content/UCF-101/UCF-101/PushUps/*.avi"))[:5]

video_tensors = {'JumpingJack': [], 'PushUps': []}
for vid in tqdm(jumpingjack_videos):
    video_tensors['JumpingJack'].append(extract_frames_from_video(vid))
for vid in tqdm(pushups_videos):
    video_tensors['PushUps'].append(extract_frames_from_video(vid))

X_train = np.concatenate([
    np.array(video_tensors['JumpingJack']),
    np.array(video_tensors['PushUps'])
], axis=0)

print("Shape:", X_train.shape)

import torch
import torch.nn as nn
import numpy as np
import cv2
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

T, H, W, C = 16, 64, 64, 3
noise_dim = 100
device = 'cuda' if torch.cuda.is_available() else 'cpu'

class VideoGenerator(nn.Module):
    def __init__(self):
        super(VideoGenerator, self).__init__()
        self.fc = nn.Linear(noise_dim, 256 * 2 * 4 * 4)
        self.net = nn.Sequential(
            nn.BatchNorm3d(256),
            nn.ReLU(),
            nn.ConvTranspose3d(256, 128, (2, 4, 4), stride=(2, 2, 2), padding=(0, 1, 1)),
            nn.BatchNorm3d(128),
            nn.ReLU(),
            nn.ConvTranspose3d(128, 64, (2, 4, 4), stride=(2, 2, 2), padding=(0, 1, 1)),
            nn.BatchNorm3d(64),
            nn.ReLU(),
            nn.ConvTranspose3d(64, 3, (2, 4, 4), stride=(2, 2, 2), padding=(0, 1, 1)),
            nn.Tanh()
        )

    def forward(self, z):
        x = self.fc(z).view(-1, 256, 2, 4, 4)
        return self.net(x)

class VideoDiscriminator(nn.Module):
    def __init__(self):
        super(VideoDiscriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Conv3d(3, 64, (4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),
            nn.LeakyReLU(0.2),
            nn.Conv3d(64, 128, (4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1)),
            nn.BatchNorm3d(128),
            nn.LeakyReLU(0.2),
            nn.AdaptiveAvgPool3d((1, 1, 1)),
            nn.Flatten(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

X_tensor = torch.tensor(X_train.transpose(0, 4, 1, 2, 3), dtype=torch.float32)
dataset = DataLoader(TensorDataset(X_tensor), batch_size=2, shuffle=True)

G = VideoGenerator().to(device)
D = VideoDiscriminator().to(device)

criterion = nn.BCELoss()
g_opt = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))
d_opt = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))

epochs = 50
for epoch in range(epochs):
    for real_batch, in dataset:
        real_batch = real_batch.to(device)
        B = real_batch.size(0)

        z = torch.randn(B, noise_dim).to(device)
        fake_videos = G(z).detach()
        d_loss = (
            criterion(D(real_batch), torch.ones(B, 1).to(device)) +
            criterion(D(fake_videos), torch.zeros(B, 1).to(device))
        ) / 2
        d_opt.zero_grad(); d_loss.backward(); d_opt.step()

        z = torch.randn(B, noise_dim).to(device)
        g_loss = criterion(D(G(z)), torch.ones(B, 1).to(device))
        g_opt.zero_grad(); g_loss.backward(); g_opt.step()

    print(f"Epoch {epoch+1}/{epochs} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}")

def save_fake_video(generator, output_path, noise_dim=100, device='cpu', fps=5):
    generator.eval()
    with torch.no_grad():
        z = torch.randn(1, noise_dim).to(device)
        fake = generator(z)[0]
        fake = (fake.permute(1, 2, 3, 0).cpu().numpy() + 1) / 2
        fake = (fake * 255).astype(np.uint8)

    T, H, W, C = fake.shape
    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (W, H))
    for frame in fake:
        writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
    writer.release()
    print(f" Saved {output_path}")

save_fake_video(G, "JumpingJack_fake.avi", device=device)
save_fake_video(G, "PushUps_fake.avi", device=device)